{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Project - Airbnb Occupancy and Revenue Prediction</h1>"
      ],
      "metadata": {
        "id": "z9eVpwz2EKMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Project Overview</h2>\n",
        "<p><b><h3>Title: Airbnb Occupancy and Revenue Analytics Using Spark - Los Angeles County</b>\n",
        "\n",
        "This project analyzes Airbnb data alongside external datasets, such as crime statistics and public transportation availability, to uncover key factors influencing occupancy rates and rental prices. By leveraging Spark for efficient large-scale data processing, the goal is to generate actionable insights to help optimize profits for Airbnb hosts in Los Angeles County.\n",
        "\n",
        "The analysis considers internal factors, such as property features and pricing strategies, alongside external elements like neighborhood safety and accessibility to transportation. The findings are intended to:\n",
        "<ul>\n",
        "(a) Refine pricing strategies to improve competitiveness</ul>\n",
        "<ul>(b) Enhance the appeal of listings by identifying targeted improvements</ul>\n",
        "<ul>(c) Guide decision-making for new property installations or renovations.</ul>\n",
        "\n",
        "\n",
        "The ultimate objective is to empower hosts to maximize occupancy rates and revenue potential through data-driven recommendations.</p>"
      ],
      "metadata": {
        "id": "KaYiUC7VER-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Dataset Metadata\n",
        "\n",
        "| Field                             | Type       | Calculated | Description                                                                 |\n",
        "|-----------------------------------|------------|------------|-----------------------------------------------------------------------------|\n",
        "| **listings_minimum_nights**       | Integer    | No         | The minimum number of nights required to book a listing.                    |\n",
        "| **listings_maximum_nights**       | Integer    | No         | The maximum number of nights allowed for booking a listing.                 |\n",
        "| **listings_minimum_minimum_nights** | Integer | Yes        | Smallest minimum nights value from the calendar (next 365 days).            |\n",
        "| **listings_maximum_minimum_nights** | Integer | Yes        | Largest minimum nights value from the calendar (next 365 days).             |\n",
        "| **listings_minimum_maximum_nights** | Integer | Yes        | Smallest maximum nights value from the calendar (next 365 days).            |\n",
        "| **listings_maximum_maximum_nights** | Integer | Yes        | Largest maximum nights value from the calendar (next 365 days).             |\n",
        "| **listings_minimum_nights_avg_ntm** | Numeric | Yes        | Average minimum nights from the calendar (next 365 days).                   |\n",
        "| **listings_maximum_nights_avg_ntm** | Numeric | Yes        | Average maximum nights from the calendar (next 365 days).                   |\n",
        "| **listings_calendar_updated**     | Date       | No         | Last updated date of the listing's calendar.                                |\n",
        "| **listings_has_availability**     | Boolean    | No         | Indicates whether the listing has future availability.                      |\n",
        "| **listings_availability_30**      | Integer    | Yes        | Number of days available for booking in the next 30 days.                   |\n",
        "| **listings_availability_60**      | Integer    | Yes        | Number of days available for booking in the next 60 days.                   |\n",
        "| **listings_availability_90**      | Integer    | Yes        | Number of days available for booking in the next 90 days.                   |\n",
        "| **listings_availability_365**     | Integer    | Yes        | Number of days available for booking in the next 365 days.                  |\n",
        "| **listings_calendar_last_scraped** | Date      | Yes        | Last date when the listing data was scraped.                                |\n",
        "| **listings_number_of_reviews**    | Integer    | No         | Total number of reviews for the listing.                                    |\n",
        "| **listings_number_of_reviews_ltm** | Integer   | Yes        | Number of reviews in the last 12 months.                                    |\n",
        "| **listings_number_of_reviews_l30d** | Integer  | Yes        | Number of reviews in the last 30 days.                                      |\n",
        "| **listings_first_review**         | Date       | Yes        | Date of the first review.                                                   |\n",
        "| **listings_last_review**          | Date       | Yes        | Date of the most recent review.                                             |\n",
        "| **listings_review_scores_rating** | Numeric    | No         | Average rating score for the listing based on guest reviews.                |\n",
        "| **listings_review_scores_accuracy** | Numeric | No         | Rating for the accuracy of the listing description.                         |\n",
        "| **listings_review_scores_cleanliness** | Numeric | No      | Rating for the cleanliness of the listing.                                  |\n",
        "| **listings_review_scores_checkin** | Numeric  | No         | Rating for the check-in process.                                            |\n",
        "| **listings_review_scores_communication** | Numeric | No    | Rating for the communication with the host.                                 |\n",
        "| **listings_review_scores_location** | Numeric | No        | Rating for the location of the listing.                                     |\n",
        "| **listings_review_scores_value**  | Numeric    | No         | Rating for the value offered by the listing.                                |\n",
        "| **listings_license**              | Text       | No         | Licensing or registration number for the listing.                           |\n",
        "| **listings_instant_bookable**     | Boolean    | No         | Indicates if a listing is bookable instantly without host approval.         |\n",
        "| **listings_calculated_host_listings_count** | Integer | Yes | Total listings by the host in the current region.                           |\n",
        "| **listings_calculated_host_listings_count_entire_homes** | Integer | Yes | Entire home/apt listings by the host.                                      |\n",
        "| **listings_calculated_host_listings_count_private_rooms** | Integer | Yes | Private room listings by the host.                                         |\n",
        "| **listings_calculated_host_listings_count_shared_rooms** | Integer | Yes | Shared room listings by the host.                                          |\n",
        "| **listings_reviews_per_month**    | Numeric    | Yes        | Average reviews per month over the lifetime of the listing.                 |\n",
        "| **listings_index_right**          | Integer    | Yes        | Internal index for geospatial joins.                                        |\n",
        "| **listings_ZIPCODE**              | Text       | No         | ZIP code for the Airbnb listing.                                            |\n",
        "| **bus_ZIPCODE**                   | Text       | No         | ZIP code area for bus stop counts.                                          |\n",
        "| **bus_bus_stops_count**           | Integer    | Yes        | Number of bus stops in the respective ZIP code area.                        |\n",
        "| **metro_ZIPCODE**                 | Text       | No         | ZIP code area for metro station counts.                                     |\n",
        "| **metro_train_stations_count**    | Integer    | Yes        | Number of metro stations in the respective ZIP code area.                   |\n",
        "| **crime_ZIPCODE**                 | Text       | No         | ZIP code area for crime data.                                               |\n",
        "| **crime_Assault and Battery**     | Integer    | Yes        | Count of assault and battery crimes in the area.                            |\n",
        "| **crime_Child-Related Crimes**    | Integer    | Yes        | Count of crimes involving children in the area.                             |\n",
        "| **crime_Cyber Crimes**            | Integer    | Yes        | Count of cybercrimes in the area.                                           |\n",
        "| **crime_Driving Offenses**        | Integer    | Yes        | Count of driving-related crimes in the area.                                |\n",
        "| **crime_Drug-Related Crimes**     | Integer    | Yes        | Count of drug-related crimes in the area.                                   |\n",
        "| **crime_Fraud and Forgery**       | Integer    | Yes        | Count of fraud and forgery incidents in the area.                           |\n",
        "| **crime_Human Trafficking**       | Integer    | Yes        | Count of human trafficking crimes in the area.                              |\n",
        "| **crime_Kidnapping and Abduction** | Integer  | Yes        | Count of kidnapping and abduction cases in the area.                        |\n",
        "| **crime_Miscellaneous Crimes**    | Integer    | Yes        | Count of miscellaneous crimes in the area.                                  |\n",
        "| **crime_Other Crimes**            | Integer    | Yes        | Count of other unspecified crimes in the area.                              |\n",
        "| **crime_Robbery and Extortion**   | Integer    | Yes        | Count of robbery and extortion cases in the area.                           |\n",
        "| **crime_Serious Crime - Homicide** | Integer   | Yes        | Count of homicide cases in the area.                                        |\n",
        "| **crime_Sexual Crimes**           | Integer    | Yes        | Count of sexual crimes in the area.                                         |\n",
        "| **crime_Theft and Burglary**      | Integer    | Yes        | Count of theft and burglary cases in the area.                              |\n",
        "| **crime_Traffic Offenses**        | Integer    | Yes        | Count of traffic offenses in the area.                                      |\n",
        "| **crime_Vandalism**               | Integer    | Yes        | Count of vandalism incidents in the area.                                   |\n",
        "| **crime_Weapons Offenses**        | Integer    | Yes        | Count of crimes involving weapons in the area.                              |\n"
      ],
      "metadata": {
        "id": "k0iB88DuFVRm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVsKT35coIv9"
      },
      "source": [
        "<h1>1. Installing all the libraries</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YJcinpOnn5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fac815-42b1-4157-c5f9-63bca8e92509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.3.0\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5 (from pyspark==3.3.0)\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764003 sha256=84d3c0f051fb5f0505975b6d9eddf1af73a7c39e4e5ee11eb977f692343b2ddd\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/9c/6c/d5200fcf351ffa39cbe09911e99703283624cd037df58070d9\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRV9QlIJTjDi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "72b3387f-33fb-4a35-8c15-5317ea65957a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-92cb1984a79e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# Importing libraries for adding zipcodes\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>We are using pyspark for this project, so we are initializing the spark session here below.</h2>"
      ],
      "metadata": {
        "id": "4aqtY0FXGVie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exRzvlVrn2X0"
      },
      "outputs": [],
      "source": [
        "# Initializing the pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .appName('Final Project')\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q3x7wS9n5jl"
      },
      "outputs": [],
      "source": [
        "# Importing all required libraries\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08mG5uTKovUN"
      },
      "outputs": [],
      "source": [
        "# Initializing all the paths\n",
        "data_path = '/content/drive/MyDrive/Big Data/'\n",
        "result_path = '/content/drive/MyDrive/Big Data/'\n",
        "file_path = '/content/drive/MyDrive/Big Data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBVqOUdKoXz8"
      },
      "source": [
        "<h1>2. Adding zipcodes to all the data files</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHE9D_ylQhmO"
      },
      "source": [
        "<h2>A. Listings</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcXwkULfpOlM"
      },
      "outputs": [],
      "source": [
        "# Reading all the files\n",
        "geojson_gdf = gpd.read_file('/content/drive/MyDrive/Big Data/data/LA_County_ZIP_Codes.geojson')\n",
        "listings_pdf = pd.read_csv('/content/drive/MyDrive/Big Data/data/listings.csv.gz')\n",
        "bus_pdf = pd.read_csv('/content/drive/MyDrive/Big Data/data/Bus_Stop_Benches.csv')\n",
        "metro_pdf = pd.read_csv('/content/drive/MyDrive/Big Data/data/Metro_Stations.csv')\n",
        "crime_pdf = pd.read_csv('/content/drive/MyDrive/Big Data/data/CrimeData.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HH8oEOKWGYk"
      },
      "outputs": [],
      "source": [
        "# Convert lat/long in CSV to a geometry column\n",
        "listings_pdf['geometry'] = listings_pdf.apply(lambda row: Point(\n",
        "    row['longitude'], row['latitude']), axis=1)\n",
        "listings_gdf = gpd.GeoDataFrame(\n",
        "    listings_pdf, geometry='geometry', crs=geojson_gdf.crs)\n",
        "\n",
        "# Perform a spatial join to join the CSV points with GeoJSON geometries\n",
        "listings_zip_df = gpd.sjoin(\n",
        "    listings_gdf, geojson_gdf, how=\"inner\", predicate='intersects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5xmRgQJp9Gd"
      },
      "outputs": [],
      "source": [
        "# Displaying shape of listings\n",
        "listings_zip_df = listings_zip_df.drop(columns=['OBJECTID', 'geometry',\n",
        "    'Shape_Length', 'Shape_Area'])\n",
        "listings_zip_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWB_p463SWCP"
      },
      "outputs": [],
      "source": [
        "listings_zip_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AU5NN3VWuzH"
      },
      "outputs": [],
      "source": [
        "listings_zip_df.to_parquet('listings_with_zipcodes.parquet', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMOWfRN-Qs31"
      },
      "source": [
        "<h2>B. Bus Stops</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CYWBg8JQED8"
      },
      "outputs": [],
      "source": [
        "# Convert lat/long in CSV to a geometry column\n",
        "bus_pdf['geometry'] = bus_pdf.apply(lambda row: Point(\n",
        "    row['LONGITUDE'], row['LATITUDE']), axis=1)\n",
        "bus_gdf = gpd.GeoDataFrame(bus_pdf, geometry='geometry',\n",
        "    crs=geojson_gdf.crs)\n",
        "\n",
        "# Perform a spatial join to join the CSV points with GeoJSON geometries\n",
        "bus_zip_df = gpd.sjoin(\n",
        "    bus_gdf, geojson_gdf, how=\"inner\", predicate='intersects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6nJVJT8FZ9i"
      },
      "outputs": [],
      "source": [
        "bus_column_list = ['OBJECTID_left', 'FID2', 'NUMBER', 'LATITUDE',\n",
        "                   'LONGITUDE', 'SITEATS', 'CITY_SITE', 'ZIPCODE']\n",
        "bus_zip_df = bus_zip_df[bus_column_list]\n",
        "bus_zip_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR-PufI-Sqvf"
      },
      "outputs": [],
      "source": [
        "bus_zip_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvHO0zBMSMG2"
      },
      "outputs": [],
      "source": [
        "bus_zip_df.to_parquet('bus_with_zipcodes.parquet', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4qKVVe-SuQC"
      },
      "source": [
        "<h2>C. Metro Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrgz90z4VqD8"
      },
      "outputs": [],
      "source": [
        "# Convert lat/long in CSV to a geometry column\n",
        "metro_pdf['geometry'] = metro_pdf.apply(lambda row: Point(\n",
        "    row['longitude'], row['latitude']), axis=1)\n",
        "\n",
        "metro_gdf = gpd.GeoDataFrame(metro_pdf, geometry='geometry',\n",
        "    crs=geojson_gdf.crs)\n",
        "\n",
        "# Perform a spatial join to join the CSV points with GeoJSON geometries\n",
        "metro_zip_df = gpd.sjoin(metro_gdf, geojson_gdf,\n",
        "    how=\"inner\", predicate='intersects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXCZa3-3V88R"
      },
      "outputs": [],
      "source": [
        "train_column_list = ['OBJECTID_left', 'source', 'cat1', 'cat2', 'Name',\n",
        "    'description', 'latitude', 'longitude', 'ZIPCODE']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFWp-W4EV_yg"
      },
      "outputs": [],
      "source": [
        "metro_zip_df = metro_zip_df[train_column_list]\n",
        "metro_zip_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLkpeAZQTYPt"
      },
      "outputs": [],
      "source": [
        "metro_zip_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVhwa4RwV8D0"
      },
      "outputs": [],
      "source": [
        "metro_zip_df.to_parquet('metro_with_zipcodes.parquet', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTq_F1CTsga"
      },
      "source": [
        "<h2>D. Crime Data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqDG4W52jbZJ"
      },
      "outputs": [],
      "source": [
        "# Convert lat/long in CSV to a geometry column\n",
        "crime_pdf['geometry'] = crime_pdf.apply(lambda row: Point(\n",
        "    row['LON'], row['LAT']), axis=1)\n",
        "crime_gdf = gpd.GeoDataFrame(crime_pdf,\n",
        "    geometry='geometry', crs=geojson_gdf.crs)\n",
        "\n",
        "# Perform a spatial join to join the CSV points with GeoJSON geometries\n",
        "crime_zip_df = gpd.sjoin(crime_gdf, geojson_gdf,\n",
        "    how=\"inner\", predicate='intersects')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAfmusChVXnk"
      },
      "outputs": [],
      "source": [
        "crime_zip_df = crime_zip_df.drop(columns=[\n",
        "    'DR_NO', 'Date Rptd', 'Rpt Dist No', 'Part 1-2', 'Mocodes',\n",
        "    'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc',\n",
        "    'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc',\n",
        "    'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION',\n",
        "    'Cross Street', 'index_right', 'OBJECTID', 'Shape_Length',\n",
        "    'Shape_Area', 'Vict Age', 'geometry']\n",
        ")\n",
        "crime_zip_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJShZ1mCUV5Y"
      },
      "outputs": [],
      "source": [
        "crime_zip_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6noHVqrpGIl"
      },
      "outputs": [],
      "source": [
        "crime_zip_df.to_parquet('crime_with_zipcodes.parquet', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IW_PmTGAWMSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaf2eUurWPOr"
      },
      "source": [
        "<h1>3. Reading all the files in Pyspark from external sources</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjP61IgPW6FL"
      },
      "outputs": [],
      "source": [
        "# Reading all parquet files\n",
        "listings_df = spark.read.parquet('listings_with_zipcodes.parquet')\n",
        "bus_df = spark.read.parquet('bus_with_zipcodes.parquet')\n",
        "metro_df = spark.read.parquet('metro_with_zipcodes.parquet')\n",
        "crime_df = spark.read.parquet('crime_with_zipcodes.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfpAKF-NZaM9"
      },
      "outputs": [],
      "source": [
        "# Displaying all data\n",
        "listings_df.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CYXuq1mZwXL"
      },
      "source": [
        "<h1>4. Data Preparation by Transformation and Merging</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6np9dOH-alHp"
      },
      "source": [
        "<h2>A. Bus data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zveEFhiNZ-8j"
      },
      "outputs": [],
      "source": [
        "# Bus data transformation\n",
        "bus_pivot_df = bus_df.groupBy(F.col('ZIPCODE')).count()\n",
        "bus_pivot_df = bus_pivot_df.withColumnRenamed('count', 'bus_stops_count')\n",
        "bus_pivot_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrwuVKsRZ_Bq"
      },
      "outputs": [],
      "source": [
        " bus_pivot_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTio6Pr4apg6"
      },
      "source": [
        "<h2>B. Train data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_BtR7qpZ_GC"
      },
      "outputs": [],
      "source": [
        "# Train data transformation\n",
        "metro_pivot_df = metro_df.groupBy(F.col('ZIPCODE')).count()\n",
        "metro_pivot_df = metro_pivot_df.withColumnRenamed('count',\n",
        "    'train_stations_count')\n",
        "metro_pivot_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k57jZVRiZ_LK"
      },
      "outputs": [],
      "source": [
        "metro_pivot_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rQ4DzzMas9q"
      },
      "source": [
        "<h2>C. Crime data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zekLbxMWbl-S"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRYyV4h1a2WK"
      },
      "outputs": [],
      "source": [
        "# Converting 'DATE OCC' to timestamp format\n",
        "crime_filtered = crime_df.withColumn(\"DATE OCC\", F.to_timestamp(\n",
        "    F.col(\"DATE OCC\"), 'MM/dd/yyyy HH:mm:ss'))\n",
        "\n",
        "# Step 3: Filter for dates between January 1, 2024, and June 30, 2024\n",
        "crime_filtered = crime_filtered.filter(\n",
        "    (F.col(\"DATE OCC\") >= '2023-12-03') &\n",
        "     (F.col(\"DATE OCC\") <= '2024-12-03'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mc_q3cma2ZU"
      },
      "outputs": [],
      "source": [
        "# Adding the Date_of_Crime column\n",
        "crime_renamed = crime_filtered.withColumn(\"Date_of_Crime\",\n",
        "    F.to_date(F.col(\"DATE OCC\")))\n",
        "\n",
        "# Showing the updated DataFrame\n",
        "crime_renamed.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc0nceqBa2fk"
      },
      "outputs": [],
      "source": [
        "# Mapping dictionary for crime codes to categories\n",
        "crime_category_mapping = {\n",
        "    '626': 'Assault and Battery', '230': 'Assault and Battery',\n",
        "    '235': 'Assault and Battery', '624': 'Assault and Battery',\n",
        "    '622': 'Assault and Battery', '623': 'Assault and Battery',\n",
        "    '627': 'Assault and Battery', '625': 'Assault and Battery',\n",
        "    '850': 'Assault and Battery', '237': 'Child-Related Crimes',\n",
        "    '922': 'Child-Related Crimes', '813': 'Child-Related Crimes',\n",
        "    '814': 'Child-Related Crimes', '870': 'Child-Related Crimes',\n",
        "    '473': 'Theft and Burglary', '474': 'Theft and Burglary',\n",
        "    '420': 'Theft and Burglary', '331': 'Theft and Burglary',\n",
        "    '341': 'Theft and Burglary', '350': 'Theft and Burglary',\n",
        "    '343': 'Theft and Burglary', '442': 'Theft and Burglary',\n",
        "    '351': 'Theft and Burglary', '450': 'Theft and Burglary',\n",
        "    '352': 'Theft and Burglary', '453': 'Theft and Burglary',\n",
        "    '210': 'Robbery and Extortion', '940': 'Robbery and Extortion',\n",
        "    '220': 'Robbery and Extortion', '654': 'Fraud and Forgery',\n",
        "    '653': 'Fraud and Forgery', '649': 'Fraud and Forgery',\n",
        "    '652': 'Fraud and Forgery', '651': 'Fraud and Forgery',\n",
        "    '670': 'Fraud and Forgery', '668': 'Fraud and Forgery',\n",
        "    '660': 'Fraud and Forgery',\n",
        "    '822': 'Human Trafficking', '921': 'Human Trafficking',\n",
        "    '821': 'Sexual Crimes', '810': 'Sexual Crimes',\n",
        "    '820': 'Sexual Crimes', '830': 'Sexual Crimes',\n",
        "    '756': 'Weapons Offenses', '761': 'Weapons Offenses',\n",
        "    '740': 'Vandalism', '745': 'Vandalism',\n",
        "    '865': 'Drug-Related Crimes',\n",
        "    '661': 'Cyber Crimes', '932': 'Cyber Crimes',\n",
        "    '438': 'Driving Offenses', '433': 'Driving Offenses',\n",
        "    '890': 'Traffic Offenses', '884': 'Traffic Offenses',\n",
        "    '470': 'Traffic Offenses', '471': 'Traffic Offenses',\n",
        "    '920': 'Kidnapping and Abduction',\n",
        "    '901': 'Kidnapping and Abduction',\n",
        "    '110': 'Serious Crime - Homicide',\n",
        "    '902': 'Miscellaneous Crimes', '900': 'Miscellaneous Crimes',\n",
        "    '903': 'Miscellaneous Crimes',\n",
        "    '880': 'Miscellaneous Crimes', '886': 'Miscellaneous Crimes',\n",
        "    '946': 'Miscellaneous Crimes',\n",
        "    '951': 'Miscellaneous Crimes', '755': 'Miscellaneous Crimes',\n",
        "    '926': 'Miscellaneous Crimes',\n",
        "    '647': 'Miscellaneous Crimes', '487': 'Miscellaneous Crimes',\n",
        "    '522': 'Miscellaneous Crimes',\n",
        "    '520': 'Miscellaneous Crimes', '510': 'Miscellaneous Crimes',\n",
        "    '668': 'Miscellaneous Crimes'\n",
        "}\n",
        "# Defining the mapping condition\n",
        "crime_category_conditions = [F.when(F.col('Crm Cd') == code, category)\n",
        "    for code, category in crime_category_mapping.items()]\n",
        "\n",
        "# Create a new column 'Crime Category' based on the mapping\n",
        "crime_category_df = crime_renamed.withColumn('Crime Category',\n",
        "    F.coalesce(*crime_category_conditions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUEH7T-2a2ic"
      },
      "outputs": [],
      "source": [
        "# Replace NULL values in the 'Crime Category' column with 'Other Crimes'\n",
        "crime_final = crime_category_df.fillna({'Crime Category': 'Other Crimes'})\n",
        "crime_final.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrKSq8DQa2kv"
      },
      "outputs": [],
      "source": [
        "# Transforming crime categories\n",
        "temp_df = crime_final.select(\"ZIPCODE\",\n",
        "    \"Date_of_Crime\", \"Crime Category\")\n",
        "crime_pivot_df = crime_final.select(\"ZIPCODE\", \"Date_of_Crime\",\n",
        "    \"Crime Category\").groupBy(\"ZIPCODE\").pivot(\n",
        "    \"Crime Category\").agg(F.count(F.lit(1)))\n",
        "crime_pivot_df = crime_pivot_df.fillna(0)\n",
        "crime_pivot_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-MWY1dAfGI3"
      },
      "source": [
        "<h2>D. Merging all data together</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R31WbgMfYAC"
      },
      "outputs": [],
      "source": [
        "# Adding aliases to listings dataframe column names\n",
        "list_old_cols = listings_df.columns\n",
        "list_new_cols = ['listings_' + col for col in list_old_cols]\n",
        "listings_merge_df = listings_df.toDF(*list_new_cols)\n",
        "listings_merge_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAgO_cYPfYG1"
      },
      "outputs": [],
      "source": [
        "# Adding aliases to bus dataframe column names\n",
        "bus_old_cols = bus_pivot_df.columns\n",
        "bus_new_cols = ['bus_' + col for col in bus_old_cols]\n",
        "bus_merge_df = bus_pivot_df.toDF(*bus_new_cols)\n",
        "bus_merge_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YNuWcjyfYNk"
      },
      "outputs": [],
      "source": [
        "# Adding aliases to train dataframe column names\n",
        "metro_old_cols = metro_pivot_df.columns\n",
        "metro_new_cols = ['metro_' + col for col in metro_old_cols]\n",
        "metro_merge_df = metro_pivot_df.toDF(*metro_new_cols)\n",
        "metro_merge_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoxQuDlwfYUf"
      },
      "outputs": [],
      "source": [
        "# Adding aliases to crime dataframe column names\n",
        "crime_old_cols = crime_pivot_df.columns\n",
        "crime_new_cols = ['crime_' + col for col in crime_old_cols]\n",
        "crime_merge_df = crime_pivot_df.toDF(*crime_new_cols)\n",
        "crime_merge_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCoCt9ndfYar"
      },
      "outputs": [],
      "source": [
        "# Merging listings and bus\n",
        "list_bus_df = listings_merge_df.join(bus_merge_df,\n",
        "    listings_merge_df.listings_ZIPCODE == bus_merge_df.bus_ZIPCODE,\n",
        "    how='left')\n",
        "list_bus_df = list_bus_df.fillna(0, subset=['bus_bus_stops_count'])\n",
        "list_bus_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGzyVe4WiMBx"
      },
      "outputs": [],
      "source": [
        "# Merging train to list_bus\n",
        "list_bus_train_df = list_bus_df.join(metro_merge_df,\n",
        "    listings_merge_df.listings_ZIPCODE == metro_merge_df.metro_ZIPCODE,\n",
        "    how='left')\n",
        "list_bus_train_df = list_bus_train_df.fillna(0,\n",
        "    subset=['metro_train_stations_count'])\n",
        "list_bus_train_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9KZoD4MifDj"
      },
      "outputs": [],
      "source": [
        "# Merging crime to list_bus_train\n",
        "crime_columns = [col for col in crime_merge_df.columns\\\n",
        "                 if col != 'crime_ZIPCODE']\n",
        "final_df = list_bus_train_df.join(crime_merge_df,\n",
        "    listings_merge_df.listings_ZIPCODE == crime_merge_df.crime_ZIPCODE,\n",
        "    how='left')\n",
        "final_df = final_df.fillna(0,\n",
        "    subset=crime_columns)\n",
        "final_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>We have merged out listings data from Airbnb Los Angeles listings with the Crime, Bus and Metro stations data based on the zipcode.</b></h3>"
      ],
      "metadata": {
        "id": "jrB7O4vyGyGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgKd_lyDXvE5"
      },
      "outputs": [],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKL50-GjqsaS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "from pyspark.sql.functions import col, regexp_replace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G99cjwOzqsxW"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.withColumn(\n",
        "    \"listings_price\",\n",
        "    regexp_replace(col(\"listings_price\"), \"[$,]\", \"\").cast(\"float\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU2slc0sqxpf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count null values in each column\n",
        "null_counts = final_df.select([\n",
        "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in final_df.columns\n",
        "])\n",
        "\n",
        "null_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiNYr11Hq7Cm"
      },
      "outputs": [],
      "source": [
        "threshold = 0.8 * 45555  # Define threshold for dropping columns\n",
        "columns_to_drop = [\n",
        "    col for col, count in zip(null_counts.columns, null_counts.collect()[0]) if count > threshold\n",
        "]\n",
        "final_df = final_df.drop(*columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlebVRmirQdr"
      },
      "outputs": [],
      "source": [
        "final_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWGMVYIvtCu2"
      },
      "outputs": [],
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'listings_listing_url', 'listings_scrape_id', 'listings_last_scraped',\n",
        "    'listings_source', 'listings_picture_url', 'listings_host_url',\n",
        "    'listings_host_name', 'listings_host_thumbnail_url', 'listings_latitude',\n",
        "    'listings_longitude', 'listings_neighbourhood_group_cleansed'\n",
        "    'listings_host_picture_url', 'listings_neighbourhood',\n",
        "    'listings_neighbourhood_group_cleansed'\n",
        "    'calendars_listing_id', 'metro_latitude',\n",
        "    'metro_longitude', 'bus_stop_NUMBER',\n",
        "    'listings_Shape_Length', 'listings_Shape_Area', 'metro_ZIPCODE',\n",
        "    'crime_ZIPCODE', 'bus_stop_ZIPCODE'\n",
        "]\n",
        "\n",
        "# Drop columns from the PySpark DataFrame\n",
        "final_df = final_df.drop(*columns_to_drop)\n",
        "\n",
        "# Show the first few rows of the updated DataFrame\n",
        "final_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSmfJuVwZut"
      },
      "outputs": [],
      "source": [
        "final_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84Quvi2pxrxR"
      },
      "outputs": [],
      "source": [
        "# Dropping irrelevant or redundant columns\n",
        "columns_to_drop = [\n",
        "    'listings_neighborhood_overview', 'listings_host_about', 'listings_host_neighbourhood',\n",
        "    'listings_license', 'listings_host_verifications'\n",
        "]\n",
        "\n",
        "final_df = final_df.drop(*columns_to_drop)\n",
        "\n",
        "# Show schema after dropping\n",
        "final_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZARcjITzS0U"
      },
      "outputs": [],
      "source": [
        "# Dropping irrelevant or redundant columns\n",
        "columns_to_drop = [\n",
        "    'listings_name']\n",
        "\n",
        "final_df = final_df.drop(*columns_to_drop)\n",
        "\n",
        "# Show schema after dropping\n",
        "final_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNCtheCYzZlX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import to_date, datediff, current_date\n",
        "\n",
        "# Convert to datetime and derive 'host_active_years'\n",
        "final_df = final_df.withColumn('listings_host_since', to_date(final_df['listings_host_since']))\n",
        "final_df = final_df.withColumn('host_active_years', (datediff(current_date(), final_df['listings_host_since']) / 365).cast('double'))\n",
        "\n",
        "# Show a few rows with the new feature\n",
        "final_df.select('listings_host_since', 'host_active_years').show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54xjFJv8zmtq"
      },
      "outputs": [],
      "source": [
        "# Drop the original 'listings_host_since' column\n",
        "final_df = final_df.drop('listings_host_since')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDYYA1HGzxMa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Remove '%' and convert to numeric\n",
        "final_df = final_df.withColumn('listings_host_response_rate', regexp_replace('listings_host_response_rate', '%', '').cast('double'))\n",
        "final_df = final_df.withColumn('listings_host_acceptance_rate', regexp_replace('listings_host_acceptance_rate', '%', '').cast('double'))\n",
        "\n",
        "# Show updated columns\n",
        "final_df.select('listings_host_response_rate', 'listings_host_acceptance_rate').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pnhy8NNz4Oc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Convert 't'/'f' to 1/0\n",
        "binary_columns = ['listings_host_is_superhost', 'listings_host_has_profile_pic', 'listings_host_identity_verified', 'listings_instant_bookable']\n",
        "for col_name in binary_columns:\n",
        "    final_df = final_df.withColumn(col_name, when(final_df[col_name] == 't', 1).otherwise(0))\n",
        "\n",
        "# Show binary column transformations\n",
        "final_df.select(*binary_columns).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrNEmXz0Cz_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, regexp_extract\n",
        "\n",
        "# Extract numeric part and assign default 1 where text doesn't specify a number\n",
        "final_df = final_df.withColumn(\n",
        "    'bathrooms_count',\n",
        "    regexp_extract('listings_bathrooms_text', r'(\\d+)', 1).cast('float')\n",
        ")\n",
        "\n",
        "# Adjust the numeric value based on the type of bathroom\n",
        "final_df = final_df.withColumn(\n",
        "    'bathrooms_count',\n",
        "    when(final_df['listings_bathrooms_text'].contains('shared'), final_df['bathrooms_count'] * 0.5)\n",
        "    .when(final_df['listings_bathrooms_text'].contains('private'), final_df['bathrooms_count'] * 1.0)\n",
        "    .otherwise(final_df['bathrooms_count'])\n",
        ")\n",
        "\n",
        "# Drop the original text column as it's now redundant\n",
        "final_df = final_df.drop('listings_bathrooms_text')\n",
        "\n",
        "# Show the result\n",
        "final_df.select('bathrooms_count').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmMz1uVZ1MKE"
      },
      "outputs": [],
      "source": [
        "# Drop the 'listings_host_location' column\n",
        "final_df = final_df.drop('listings_host_location')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFSqtn6p1gHS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Map response times into numerical categories\n",
        "final_df = final_df.withColumn(\n",
        "    'response_time_category',\n",
        "    when(final_df['listings_host_response_time'] == 'within an hour', 1)\n",
        "    .when(final_df['listings_host_response_time'] == 'within a few hours', 2)\n",
        "    .when(final_df['listings_host_response_time'] == 'within a day', 3)\n",
        "    .when(final_df['listings_host_response_time'] == 'a few days or more', 4)\n",
        "    .otherwise(5)  # For NULL or undefined values\n",
        ")\n",
        "\n",
        "# Drop the original column\n",
        "final_df = final_df.drop('listings_host_response_time')\n",
        "\n",
        "# Show the transformed column\n",
        "final_df.select('response_time_category').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMKUxbGk2SJb"
      },
      "outputs": [],
      "source": [
        "# Count the number of distinct values in 'listings_neighbourhood_cleansed'\n",
        "distinct_count = final_df.select('listings_neighbourhood_cleansed').distinct().count()\n",
        "\n",
        "print(f\"Number of distinct neighborhoods: {distinct_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhMLxfkv4ZdF"
      },
      "outputs": [],
      "source": [
        "# Drop 'listings_neighbourhood_cleansed' since ZIPCODE is available\n",
        "final_df = final_df.drop('listings_neighbourhood_cleansed')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of distinct values in 'listings_neighbourhood_cleansed'\n",
        "distinct_count = final_df.select('listings_property_type').distinct().count()\n",
        "\n",
        "print(f\"Number of distinct property type: {distinct_count}\")\n"
      ],
      "metadata": {
        "id": "13OP41dD29qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC2rBncB48yV"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.withColumn(\n",
        "    'property_type_group',\n",
        "    F.when(final_df['listings_property_type'].rlike('Private room'), 'Private Room')\n",
        "    .when(final_df['listings_property_type'].rlike('Entire|Townhouse'), 'Entire Unit')\n",
        "    .when(final_df['listings_property_type'].rlike('Shared room'), 'Shared Accommodation')\n",
        "    .when(final_df['listings_property_type'].rlike('Yurt|Treehouse|Lighthouse|Cave|Castle|Dome|Tower'), 'Unique Stays')\n",
        "    .when(final_df['listings_property_type'].rlike('Boutique|Hotel|Hostel|Serviced apartment'), 'Boutique & Hotels')\n",
        "    .when(final_df['listings_property_type'].rlike('Campsite|Tipi|Shepherd’s hut|Tent|Hut'), 'Nature-Based Stays')\n",
        "    .when(final_df['listings_property_type'].rlike('Farm stay|Ranch|Barn'), 'Farm & Ranch Stays')\n",
        "    .when(final_df['listings_property_type'].rlike('Camper|RV|Bus|Train|Boat|Houseboat'), 'Vehicles as Accommodation')\n",
        "    .when(final_df['listings_property_type'].rlike('Resort|Villa|Bungalow'), 'Luxury & Resort Stays')\n",
        "    .when(final_df['listings_property_type'].rlike('Casa|Minsu|Earthen home|Cycladic home'), 'Cultural & Local Experiences')\n",
        "    .otherwise('Other')  # Catch-all for unclassified property types\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmQIwvb35JpV"
      },
      "outputs": [],
      "source": [
        "# Drop the original 'listings_property_type' column\n",
        "final_df = final_df.drop('listings_property_type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY8MEm3L5SuO"
      },
      "outputs": [],
      "source": [
        "# Count the occurrences of each property type group\n",
        "final_df.groupBy('property_type_group').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Y6VCdf5W7i"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Get unique values from 'property_type_group'\n",
        "property_types = final_df.select('property_type_group').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Create a binary column for each property type\n",
        "for property_type in property_types:\n",
        "    col_name = f'property_type_{property_type.replace(\" \", \"_\")}'  # Replace spaces for valid column names\n",
        "    final_df = final_df.withColumn(col_name, when(final_df['property_type_group'] == property_type, 1).otherwise(0))\n",
        "\n",
        "# Drop the original 'property_type_group' column\n",
        "final_df = final_df.drop('property_type_group')\n",
        "\n",
        "# Show the new one-hot encoded columns\n",
        "final_df.select([f'property_type_{property_type.replace(\" \", \"_\")}' for property_type in property_types]).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz3fmUvT5lm_"
      },
      "outputs": [],
      "source": [
        "# Check distinct values in 'listings_room_type'\n",
        "final_df.select('listings_room_type').distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAf_cj3m5x-x"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "room_types = ['Shared room', 'Hotel room', 'Entire home/apt', 'Private room']\n",
        "for room_type in room_types:\n",
        "    col_name = f'room_type_{room_type.replace(\" \", \"_\")}'  # Replace spaces with underscores\n",
        "    final_df = final_df.withColumn(col_name, when(final_df['listings_room_type'] == room_type, 1).otherwise(0))\n",
        "\n",
        "# Drop the original 'listings_room_type' column\n",
        "final_df = final_df.drop('listings_room_type')\n",
        "\n",
        "# Show the one-hot encoded columns\n",
        "final_df.select([f'room_type_{room_type.replace(\" \", \"_\")}' for room_type in room_types]).show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQvs3E8754g_"
      },
      "outputs": [],
      "source": [
        "# Show a sample of 'listings_amenities'\n",
        "final_df.select('listings_amenities').show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.withColumn('listings_amenities', F.lower(col('listings_amenities')))"
      ],
      "metadata": {
        "id": "eKjxf8I6TgxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 27 categories and their associated keywords\n",
        "categories = {\n",
        "    \"free_wifi_and_internet\": [\"wifi\", \"internet\"],\n",
        "    \"outdoor_spaces\": [\"patio\", \"balcony\", \"backyard\", \"garden\", \"terrace\", \"views\"],\n",
        "    \"swimming_facilities\": [\"pool\", \"hot tub\", \"jacuzzi\"],\n",
        "    \"kitchen_essentials\": [\"kitchen\", \"microwave\", \"stove\", \"oven\", \"refrigerator\"],\n",
        "    \"laundry_facilities\": [\"washer\", \"dryer\", \"iron\", \"ironing board\"],\n",
        "    \"heating_and_cooling\": [\"air conditioning\", \"heating\", \"fan\"],\n",
        "    \"safety_and_security\": [\"carbon monoxide alarm\", \"smoke detector\", \"first aid kit\"],\n",
        "    \"entertainment\": [\"tv\", \"streaming\", \"cable\"],\n",
        "    \"free_parking\": [\"free parking\"],\n",
        "    \"parking_facilities\": [\"garage\", \"paid parking\"],\n",
        "    \"pet_friendly\": [\"pets allowed\", \"dog-friendly\"],\n",
        "    \"accessibility_features\": [\"elevator\", \"wheelchair access\"],\n",
        "    \"fitness_and_gym_equipment\": [\"gym\", \"fitness\", \"workout equipment\"],\n",
        "    \"child_friendly_amenities\": [\"crib\", \"high chair\", \"playpen\"],\n",
        "    \"essentials\": [\"towels\", \"shampoo\", \"soap\", \"toiletries\"],\n",
        "    \"workspace_features\": [\"desk\", \"workspace\", \"ergonomic chair\"],\n",
        "    \"barbecue_and_grills\": [\"grill\", \"barbecue\"],\n",
        "    \"luxury_features\": [\"private pool\", \"sauna\", \"jacuzzi\"],\n",
        "    \"recreational_features\": [\"game room\", \"table tennis\", \"ping pong\"],\n",
        "    \"transport_features\": [\"airport shuttle\", \"bike rental\"],\n",
        "    \"scenic_views\": [\"mountain view\", \"sea view\", \"city view\"],\n",
        "    \"shared_spaces\": [\"shared spaces\", \"communal area\"],\n",
        "    \"technology_and_smart_home\": [\"smart lock\", \"smart thermostat\"],\n",
        "    \"dining_area\": [\"dining table\", \"breakfast nook\"],\n",
        "    \"event_friendly_spaces\": [\"event space\", \"meeting room\"],\n",
        "    \"green_spaces\": [\"garden\", \"outdoor plants\"],\n",
        "    \"luxury_entertainment\": [\"home theater\", \"cinema\", \"pool table\"],\n",
        "    \"special_accommodations\": [\"allergy-friendly\", \"vegan-friendly\"],\n",
        "}\n",
        "\n",
        "# Add a new column for each category with the count of matches in \"amenities\"\n",
        "for category, keywords in categories.items():\n",
        "    condition = F.lit(0)  # Initialize condition as a numeric column with 0\n",
        "    for keyword in keywords:\n",
        "        condition = condition + F.when(F.lower(F.col(\"listings_amenities\")).contains(keyword.lower()), 1).otherwise(0)\n",
        "    final_df = final_df.withColumn(category, condition)\n",
        "\n",
        "# Show the updated DataFrame\n",
        "final_df.select(\"listings_amenities\", *categories.keys()).show(truncate=False)"
      ],
      "metadata": {
        "id": "7ZLOw_LpTLwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYoU43O36HyY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import size, split\n",
        "\n",
        "# Count total amenities\n",
        "final_df = final_df.withColumn('total_amenities', size(split('listings_amenities', ',')))\n",
        "\n",
        "# Show the new column\n",
        "final_df.select('listings_amenities', 'total_amenities').show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-BVeaWS6s1D"
      },
      "outputs": [],
      "source": [
        "# Drop the 'listings_amenities' column\n",
        "final_df = final_df.drop('listings_amenities')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuNNYQDw6zuf"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop('listings_calendar_last_scraped')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKcC0PH36-qV"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "final_df = final_df.withColumn(\n",
        "    'has_availability',\n",
        "    when(final_df['listings_has_availability'] == 't', 1).otherwise(0)\n",
        ")\n",
        "\n",
        "# Drop the original column\n",
        "final_df = final_df.drop('listings_has_availability')\n",
        "\n",
        "# Show the new column\n",
        "final_df.select('has_availability').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omd2G2iE7CfQ"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.drop('bus_ZIPCODE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHOLwVYP7XY3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Convert to date format\n",
        "final_df = final_df.withColumn('listings_first_review', to_date(final_df['listings_first_review']))\n",
        "final_df = final_df.withColumn('listings_last_review', to_date(final_df['listings_last_review']))\n",
        "\n",
        "# Show the converted columns\n",
        "final_df.select('listings_first_review', 'listings_last_review').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riaDGX0n7e_a"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import datediff, current_date\n",
        "\n",
        "# Calculate days since last review\n",
        "final_df = final_df.withColumn('days_since_last_review', datediff(current_date(), final_df['listings_last_review']))\n",
        "\n",
        "# Calculate duration between first and last review\n",
        "final_df = final_df.withColumn('review_activity_duration', datediff(final_df['listings_last_review'], final_df['listings_first_review']))\n",
        "\n",
        "# Show the new columns\n",
        "final_df.select('listings_first_review', 'listings_last_review', 'days_since_last_review', 'review_activity_duration').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0PV69ru7kBE"
      },
      "outputs": [],
      "source": [
        "# Drop the original columns\n",
        "final_df = final_df.drop('listings_first_review', 'listings_last_review')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZognl7j7qNa"
      },
      "outputs": [],
      "source": [
        "final_df.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go0FlupG7rI6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count null values for each column\n",
        "null_counts = final_df.select([sum(col(c).isNull().cast('int')).alias(c) for c in final_df.columns])\n",
        "\n",
        "# Show the results\n",
        "null_counts.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uXb9AmR8B2Z"
      },
      "outputs": [],
      "source": [
        "# Drop columns with excessive nulls if needed\n",
        "final_df = final_df.drop('listings_review_scores_rating', 'listings_review_scores_accuracy')  # Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgIcpz_N8TmB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Check unique values to assess the column\n",
        "final_df.select('bathrooms_count').distinct().show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E68KF8yF8ydq"
      },
      "outputs": [],
      "source": [
        "# Impute values for columns based on specified strategies\n",
        "columns_to_impute_mean = [\n",
        "    'listings_host_response_rate', 'listings_host_acceptance_rate',\n",
        "    'listings_review_scores_cleanliness', 'listings_review_scores_checkin',\n",
        "    'listings_review_scores_communication', 'listings_review_scores_location',\n",
        "    'listings_review_scores_value'\n",
        "]\n",
        "\n",
        "# Impute mean for specified columns\n",
        "for col_name in columns_to_impute_mean:\n",
        "    mean_value = final_df.select(col_name).groupBy().avg(col_name).first()[0]\n",
        "    final_df = final_df.fillna({col_name: mean_value})\n",
        "\n",
        "# Impute 0 for specified columns\n",
        "columns_to_impute_zero = [\n",
        "    'listings_host_listings_count', 'listings_host_total_listings_count',\n",
        "    'listings_beds', 'listings_reviews_per_month', 'host_active_years',\n",
        "    'bathrooms_count'\n",
        "]\n",
        "\n",
        "final_df = final_df.fillna({col_name: 0 for col_name in columns_to_impute_zero})\n",
        "\n",
        "# Impute -1 for 'days_since_last_review'\n",
        "final_df = final_df.fillna({'days_since_last_review': -1})\n",
        "\n",
        "# Confirm imputation\n",
        "final_df.select([\n",
        "    sum(col(c).isNull().cast('int')).alias(c) for c in columns_to_impute_mean + columns_to_impute_zero + ['days_since_last_review']\n",
        "]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGkv2jQU_-Rj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Cast the column to integer type\n",
        "final_df = final_df.withColumn('listings_ZIPCODE', col('listings_ZIPCODE').cast('int'))\n",
        "\n",
        "# Verify the data type\n",
        "final_df.printSchema()\n",
        "\n",
        "# Show the first few rows to confirm the change\n",
        "final_df.select('listings_ZIPCODE').show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSvzB6JCA_4c"
      },
      "outputs": [],
      "source": [
        "final_df = final_df.fillna({'review_activity_duration': -1})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with null values in any column\n",
        "final_df_cleaned = final_df.dropna()\n",
        "final_df_cleaned_listings = final_df.dropna()"
      ],
      "metadata": {
        "id": "0sEiCfuwAgQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the resulting DataFrame\n",
        "final_df_cleaned.show(5)\n",
        "\n",
        "# Count the rows before and after dropping nulls\n",
        "print(f\"Rows before dropping nulls: {final_df.count()}\")\n",
        "print(f\"Rows after dropping nulls: {final_df_cleaned.count()}\")"
      ],
      "metadata": {
        "id": "o9oqLaAwEsEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = [col for col in final_df.columns if final_df.schema[col].dataType.typeName() in ['double', 'long', 'float', 'integer']]\n",
        "\n",
        "# Function to calculate correlations\n",
        "def calculate_correlations(df, target_variable, numerical_cols):\n",
        "    # Assemble features excluding the target variable\n",
        "    assembler = VectorAssembler(inputCols=[col for col in numerical_cols if col != target_variable], outputCol=\"features\")\n",
        "    df_features = assembler.transform(df).select(\"features\", target_variable)\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    correlation_matrix = Correlation.corr(df_features, \"features\", \"pearson\").head()[0].toArray()\n",
        "\n",
        "    # Extract correlations\n",
        "    correlations = {numerical_cols[i]: correlation_matrix[-1, i] for i in range(len(numerical_cols) - 1)}\n",
        "\n",
        "    # Sort correlations by absolute value for feature importance\n",
        "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "    return sorted_correlations\n",
        "\n",
        "# Calculate correlations for `listings_availability_30`\n",
        "target_variable_1 = \"listings_availability_30\"\n",
        "correlations_availability_30 = calculate_correlations(final_df, target_variable_1, numerical_cols)\n",
        "\n",
        "# Calculate correlations for `listings_price`\n",
        "target_variable_2 = \"listings_price\"\n",
        "correlations_price = calculate_correlations(final_df, target_variable_2, numerical_cols)\n",
        "\n",
        "# Print results\n",
        "print(\"Feature Importance for listings_availability_30:\")\n",
        "for feature, importance in correlations_availability_30:\n",
        "    print(f\"{feature}: {importance}\")\n",
        "\n",
        "print(\"\\nFeature Importance for listings_price:\")\n",
        "for feature, importance in correlations_price:\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "id": "4At7189-BFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to plot sorted feature importance\n",
        "def plot_sorted_feature_importance_fixed(correlations, title):\n",
        "    # Separate positive and negative correlations\n",
        "    positive_correlations = [item for item in correlations if item[1] > 0.15]\n",
        "    negative_correlations = [item for item in correlations if item[1] <= -0.07]\n",
        "\n",
        "    # Sort each group\n",
        "    positive_correlations = sorted(positive_correlations, key=lambda x: x[1], reverse=True)\n",
        "    negative_correlations = sorted(negative_correlations, key=lambda x: x[1], reverse = True)\n",
        "\n",
        "    # Combine groups in the desired \"S\" shape order\n",
        "    correlations_sorted = positive_correlations + negative_correlations\n",
        "\n",
        "    # Unzip features and their importance values\n",
        "    features, importance = zip(*correlations_sorted)\n",
        "\n",
        "    # Create a vertical bar chart\n",
        "    plt.figure(figsize=(30, len(features) * 0.25))\n",
        "    plt.barh(features, importance, color=np.where(np.array(importance) > 0, 'skyblue', 'salmon'))\n",
        "    plt.axvline(0, color='black', linewidth=0.7, linestyle='--')  # Add a vertical line at 0\n",
        "    plt.xlabel(\"Correlation Coefficients\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to place positive at the top\n",
        "    plt.tight_layout()  # Adjust layout to fit all features\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wdggatACx7f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>A. Random Forest</h2>"
      ],
      "metadata": {
        "id": "TNKUwANKycdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.drop('features_price')\n",
        "\n",
        "# Step 1: Log Transformation for listings_price\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.withColumn(\"log_listings_price\", F.log(F.col(\"listings_price\")))\n",
        "\n",
        "feature_cols_price = [\n",
        "    col for col in final_df_cleaned_listings.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for availability prediction\n",
        "assembler_price = VectorAssembler(inputCols=feature_cols_price, outputCol=\"features_price\")\n",
        "final_df_cleaned_listings = assembler_price.transform(final_df_cleaned_listings)\n",
        "\n",
        "# Step 3: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned_listings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "# Step 5: Model for listings_availability_30 (without transformation)\n",
        "rf_price = RandomForestRegressor(featuresCol=\"features_price\", labelCol=\"log_listings_price\", numTrees=100)\n",
        "rf_price_model = rf_price.fit(train_df)\n",
        "predictions_price = rf_price_model.transform(test_df)\n",
        "\n",
        "# Evaluate listings_availability_90 model\n",
        "evaluator_rmse_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_price = evaluator_rmse_price.evaluate(predictions_price)\n",
        "r2_price = evaluator_r2_price.evaluate(predictions_price)\n",
        "\n",
        "print(f\"Price Model - RMSE: {rmse_price}\")\n",
        "print(f\"Price Model - R^2: {r2_price}\")\n",
        "\n",
        "# Inverse transformation for predictions\n",
        "predictions_price = predictions_price.withColumn(\"price_original\", F.exp(F.col(\"prediction\")))\n",
        "\n",
        "# Show original prices and predicted prices\n",
        "# Show availability predictions\n",
        "print(\"Price Predictions:\")\n",
        "predictions_price.select(\"listings_price\", \"price_original\").show(5)"
      ],
      "metadata": {
        "id": "ttihdPzABdfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_cleaned = final_df_cleaned.drop('features_availability')\n",
        "\n",
        "feature_cols_availability = [\n",
        "    col for col in final_df_cleaned.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_availability_30\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for availability prediction\n",
        "assembler_availability = VectorAssembler(inputCols=feature_cols_availability, outputCol=\"features_availability\")\n",
        "final_df_cleaned = assembler_availability.transform(final_df_cleaned)\n",
        "\n",
        "# Step 3: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "\n",
        "# Step 5: Model for listings_availability_30 (without transformation)\n",
        "rf_availability = RandomForestRegressor(featuresCol=\"features_availability\", labelCol=\"listings_availability_30\", numTrees=100)\n",
        "rf_availability_model = rf_availability.fit(train_df)\n",
        "predictions_availability = rf_availability_model.transform(test_df)\n",
        "\n",
        "# Evaluate listings_availability_90 model\n",
        "evaluator_rmse_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_availability = evaluator_rmse_avail.evaluate(predictions_availability)\n",
        "r2_availability = evaluator_r2_avail.evaluate(predictions_availability)\n",
        "\n",
        "print(f\"Availability Model - RMSE: {rmse_availability}\")\n",
        "print(f\"Availability Model - R^2: {r2_availability}\")\n",
        "\n",
        "# Show availability predictions\n",
        "print(\"Availability Predictions:\")\n",
        "predictions_availability.select(\"prediction\", \"listings_availability_30\").show(5)"
      ],
      "metadata": {
        "id": "RvdtUiK0BfxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>B. Linear Regression</h2>"
      ],
      "metadata": {
        "id": "xlugbv1Oyq9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Drop 'features_price' column if it exists\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.drop('features_price')\n",
        "\n",
        "# Step 1: Log Transformation for listings_price\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.withColumn(\"log_listings_price\", F.log(F.col(\"listings_price\")))\n",
        "\n",
        "# Step 2: Define feature columns, excluding specific ones\n",
        "feature_cols_price = [\n",
        "    col for col in final_df_cleaned_listings.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for prediction\n",
        "assembler_price = VectorAssembler(inputCols=feature_cols_price, outputCol=\"features_price\")\n",
        "final_df_cleaned_listings = assembler_price.transform(final_df_cleaned_listings)\n",
        "\n",
        "# Step 3: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned_listings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 4: Train Linear Regression Model\n",
        "lr_price = LinearRegression(featuresCol=\"features_price\", labelCol=\"log_listings_price\", maxIter=100, regParam=0.1, elasticNetParam=0.8)\n",
        "lr_price_model = lr_price.fit(train_df)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "predictions_price = lr_price_model.transform(test_df)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "evaluator_rmse_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_price = evaluator_rmse_price.evaluate(predictions_price)\n",
        "r2_price = evaluator_r2_price.evaluate(predictions_price)\n",
        "\n",
        "print(f\"Linear Regression Model - RMSE: {rmse_price}\")\n",
        "print(f\"Linear Regression Model - R^2: {r2_price}\")\n",
        "\n",
        "# Step 7: Inverse transformation for predictions\n",
        "predictions_price = predictions_price.withColumn(\"price_original\", F.exp(F.col(\"prediction\")))\n",
        "\n",
        "# Step 8: Display original prices and predicted prices\n",
        "print(\"Price Predictions:\")\n",
        "predictions_price.select(\"listings_price\", \"price_original\").show(5, truncate=False)"
      ],
      "metadata": {
        "id": "JRCZ8DSbBlNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'features_availability' column if it exists\n",
        "final_df_cleaned = final_df_cleaned.drop('features_availability')\n",
        "\n",
        "# Step 1: Define feature columns\n",
        "feature_cols_availability = [\n",
        "    col for col in final_df_cleaned.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_availability_30\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for availability prediction\n",
        "assembler_availability = VectorAssembler(inputCols=feature_cols_availability, outputCol=\"features_availability\")\n",
        "final_df_cleaned = assembler_availability.transform(final_df_cleaned)\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 3: Train Linear Regression Model\n",
        "lr_availability = LinearRegression(featuresCol=\"features_availability\", labelCol=\"listings_availability_30\", maxIter=100, regParam=0.1, elasticNetParam=0.8)\n",
        "lr_availability_model = lr_availability.fit(train_df)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "predictions_availability = lr_availability_model.transform(test_df)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "evaluator_rmse_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_availability = evaluator_rmse_avail.evaluate(predictions_availability)\n",
        "r2_availability = evaluator_r2_avail.evaluate(predictions_availability)\n",
        "\n",
        "print(f\"Availability Model - RMSE: {rmse_availability}\")\n",
        "print(f\"Availability Model - R^2: {r2_availability}\")\n",
        "\n",
        "# Step 6: Show availability predictions\n",
        "print(\"Availability Predictions:\")\n",
        "predictions_availability.select(\"prediction\", \"listings_availability_30\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "XcxUhrpRBpJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>C. Gradient Boosted Tree</h2>"
      ],
      "metadata": {
        "id": "JkafLk4zyyWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "# Drop 'features_price' column if it exists\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.drop('features_price')\n",
        "\n",
        "# Step 1: Log Transformation for listings_price\n",
        "final_df_cleaned_listings = final_df_cleaned_listings.withColumn(\"log_listings_price\", F.log(F.col(\"listings_price\")))\n",
        "\n",
        "# Step 2: Define feature columns\n",
        "feature_cols_price = [\n",
        "    col for col in final_df_cleaned_listings.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for prediction\n",
        "assembler_price = VectorAssembler(inputCols=feature_cols_price, outputCol=\"features_price\")\n",
        "final_df_cleaned_listings = assembler_price.transform(final_df_cleaned_listings)\n",
        "\n",
        "# Step 3: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned_listings.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 4: Train Gradient-Boosted Tree Regressor\n",
        "gbt_price = GBTRegressor(featuresCol=\"features_price\", labelCol=\"log_listings_price\", maxIter=100)\n",
        "gbt_price_model = gbt_price.fit(train_df)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "predictions_price = gbt_price_model.transform(test_df)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "evaluator_rmse_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_price = RegressionEvaluator(labelCol=\"log_listings_price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_price = evaluator_rmse_price.evaluate(predictions_price)\n",
        "r2_price = evaluator_r2_price.evaluate(predictions_price)\n",
        "\n",
        "print(f\"GBT Price Model - RMSE: {rmse_price}\")\n",
        "print(f\"GBT Price Model - R^2: {r2_price}\")\n",
        "\n",
        "# Step 7: Inverse transformation for predictions\n",
        "predictions_price = predictions_price.withColumn(\"price_original\", F.exp(F.col(\"prediction\")))\n",
        "\n",
        "# Step 8: Display original prices and predicted prices\n",
        "print(\"Price Predictions:\")\n",
        "predictions_price.select(\"listings_price\", \"price_original\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "-RxKx_pdBrGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop 'features_availability' column if it exists\n",
        "final_df_cleaned = final_df_cleaned.drop('features_availability')\n",
        "\n",
        "# Step 1: Define feature columns\n",
        "feature_cols_availability = [\n",
        "    col for col in final_df_cleaned.columns\n",
        "    if col not in [\"listings_price\", \"log_listings_price\", \"listings_id\", \"listings_host_id\", \"listings_availability_30\", \"listings_neighbourhood_group_cleansed\", \"listings_host_picture_url\"]\n",
        "]\n",
        "\n",
        "# Assemble features for availability prediction\n",
        "assembler_availability = VectorAssembler(inputCols=feature_cols_availability, outputCol=\"features_availability\")\n",
        "final_df_cleaned = assembler_availability.transform(final_df_cleaned)\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "train_df, test_df = final_df_cleaned.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 3: Train Gradient-Boosted Tree Regressor\n",
        "gbt_availability = GBTRegressor(featuresCol=\"features_availability\", labelCol=\"listings_availability_30\", maxIter=100)\n",
        "gbt_availability_model = gbt_availability.fit(train_df)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "predictions_availability = gbt_availability_model.transform(test_df)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "evaluator_rmse_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2_avail = RegressionEvaluator(labelCol=\"listings_availability_30\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "rmse_availability = evaluator_rmse_avail.evaluate(predictions_availability)\n",
        "r2_availability = evaluator_r2_avail.evaluate(predictions_availability)\n",
        "\n",
        "print(f\"GBT Availability Model - RMSE: {rmse_availability}\")\n",
        "print(f\"GBT Availability Model - R^2: {r2_availability}\")\n",
        "\n",
        "# Step 6: Display availability predictions\n",
        "print(\"Availability Predictions:\")\n",
        "predictions_availability.select(\"prediction\", \"listings_availability_30\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "L3eQO31oB41v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b><h2>Revenue Prediction!</h2></b>"
      ],
      "metadata": {
        "id": "WUXVpnkQBRfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok pandas openpyxl pyspark\n"
      ],
      "metadata": {
        "id": "xs-5YkDz5pDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Group by 'listings_ZIPCODE' and calculate average of (90 - prediction)\n",
        "avg_predictions_by_zip = predictions_availability.groupBy('listings_ZIPCODE').agg(\n",
        "    F.avg(90 - F.col('prediction')).alias('avg_prediction')\n",
        ")\n",
        "\n",
        "# Sort by avg_prediction in descending order\n",
        "avg_predictions_by_zip = avg_predictions_by_zip.orderBy(F.col('avg_prediction').desc())\n",
        "\n",
        "# Show the results\n",
        "avg_predictions_by_zip.show(10)"
      ],
      "metadata": {
        "id": "z1rigiC4qM-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash\n"
      ],
      "metadata": {
        "id": "oP5qKHaXVutT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the data\n",
        "columns_to_select = [\n",
        "    \"listings_ZIPCODE\",\n",
        "    \"property_type_Entire_Unit\",\n",
        "    \"property_type_Cultural_&_Local_Experiences\",\n",
        "    \"property_type_Other\",\n",
        "    \"property_type_Vehicles_as_Accommodation\",\n",
        "    \"property_type_Unique_Stays\",\n",
        "    \"property_type_Farm_&_Ranch_Stays\",\n",
        "    \"property_type_Shared_Accommodation\",\n",
        "    \"property_type_Nature-Based_Stays\",\n",
        "    \"property_type_Private_Room\",\n",
        "    \"room_type_Shared_room\",\n",
        "    \"room_type_Hotel_room\",\n",
        "    \"room_type_Entire_home/apt\",\n",
        "    \"room_type_Private_room\",\n",
        "    \"bathrooms_count\",\n",
        "    \"listings_price\",\n",
        "    \"listings_availability_90\"\n",
        "]\n",
        "\n",
        "df = final_df.select(*columns_to_select).toPandas()\n",
        "\n",
        "# Add a 'Property Type' column\n",
        "df['Property Type'] = df[[\n",
        "    \"property_type_Entire_Unit\",\n",
        "    \"property_type_Cultural_&_Local_Experiences\",\n",
        "    \"property_type_Other\",\n",
        "    \"property_type_Vehicles_as_Accommodation\",\n",
        "    \"property_type_Unique_Stays\",\n",
        "    \"property_type_Farm_&_Ranch_Stays\",\n",
        "    \"property_type_Shared_Accommodation\",\n",
        "    \"property_type_Nature-Based_Stays\",\n",
        "    \"property_type_Private_Room\"\n",
        "]].idxmax(axis=1)\n",
        "\n",
        "# Add a 'Room Type' column\n",
        "df['Room Type'] = df[[\n",
        "    \"room_type_Shared_room\",\n",
        "    \"room_type_Hotel_room\",\n",
        "    \"room_type_Entire_home/apt\",\n",
        "    \"room_type_Private_room\"\n",
        "]].idxmax(axis=1)\n",
        "\n",
        "# Remove prefixes for cleaner names\n",
        "df['Property Type'] = df['Property Type'].str.replace('property_type_', '', regex=False)\n",
        "df['Room Type'] = df['Room Type'].str.replace('room_type_', '', regex=False)\n",
        "\n",
        "# Calculate Revenue\n",
        "df['Revenue'] = (90 - df['listings_availability_90']) * df['listings_price']\n",
        "\n",
        "# Initialize Dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# App layout\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Dynamic Pivot Table by ZIP Code\"),\n",
        "    html.Label(\"Select a ZIP Code:\"),\n",
        "    dcc.Dropdown(\n",
        "        id=\"zip-dropdown\",\n",
        "        options=[{\"label\": str(zip_code), \"value\": zip_code} for zip_code in df[\"listings_ZIPCODE\"].unique()],\n",
        "        value=df[\"listings_ZIPCODE\"].unique()[0]\n",
        "    ),\n",
        "    html.Div(id=\"pivot-table\")\n",
        "])\n",
        "\n",
        "# Callback to update pivot table based on selected ZIP Code\n",
        "@app.callback(\n",
        "    Output(\"pivot-table\", \"children\"),\n",
        "    [Input(\"zip-dropdown\", \"value\")]\n",
        ")\n",
        "def update_pivot_table(selected_zip):\n",
        "    filtered_df = df[df[\"listings_ZIPCODE\"] == selected_zip]\n",
        "    pivot_table = pd.pivot_table(\n",
        "        filtered_df,\n",
        "        values=\"Revenue\",\n",
        "        index=[\"Property Type\", \"Room Type\", \"bathrooms_count\"],\n",
        "        aggfunc=\"mean\"\n",
        "    ).reset_index()\n",
        "\n",
        "    pivot_table.columns = [\"Property Type\", \"Room Type\", \"Bathroom Count\", \"Average Revenue Generated\"]\n",
        "\n",
        "    # Limit table to top rows for compactness\n",
        "    compact_table = pivot_table.head(10)  # Show only the first 10 rows\n",
        "\n",
        "    return html.Div([\n",
        "        html.H3(f\"Pivot Table for ZIP Code: {selected_zip}\"),\n",
        "        dcc.Graph(\n",
        "            figure={\n",
        "                \"data\": [\n",
        "                    {\n",
        "                        \"type\": \"table\",\n",
        "                        \"header\": {\n",
        "                            \"values\": list(compact_table.columns),\n",
        "                            \"fill\": {\"color\": \"lightgrey\"},\n",
        "                            \"align\": \"center\",\n",
        "                            \"font\": {\"size\": 10}  # Adjust font size\n",
        "                        },\n",
        "                        \"cells\": {\n",
        "                            \"values\": [compact_table[col] for col in compact_table.columns],\n",
        "                            \"fill\": {\"color\": \"white\"},\n",
        "                            \"align\": \"center\",\n",
        "                            \"height\": 20  # Adjust cell height\n",
        "                        }\n",
        "                    }\n",
        "                ],\n",
        "                \"layout\": {\n",
        "                    \"autosize\": False,\n",
        "                    \"width\": 700,  # Adjust table width\n",
        "                    \"height\": 400  # Adjust table height\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(debug=True, port=8051)\n"
      ],
      "metadata": {
        "id": "UhvZRfEvAolH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "from dash.dash_table import DataTable\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare the data (optimized for performance)\n",
        "columns_to_select = [\n",
        "    \"listings_ZIPCODE\",\n",
        "    \"property_type_Entire_Unit\",\n",
        "    \"property_type_Cultural_&_Local_Experiences\",\n",
        "    \"property_type_Other\",\n",
        "    \"property_type_Vehicles_as_Accommodation\",\n",
        "    \"property_type_Unique_Stays\",\n",
        "    \"property_type_Farm_&_Ranch_Stays\",\n",
        "    \"property_type_Shared_Accommodation\",\n",
        "    \"property_type_Nature-Based_Stays\",\n",
        "    \"property_type_Private_Room\",\n",
        "    \"room_type_Shared_room\",\n",
        "    \"room_type_Hotel_room\",\n",
        "    \"room_type_Entire_home/apt\",\n",
        "    \"room_type_Private_room\",\n",
        "    \"bathrooms_count\",\n",
        "    \"listings_price\",\n",
        "    \"listings_availability_90\"\n",
        "]\n",
        "\n",
        "# Sample data conversion for testing\n",
        "df = final_df.select(*columns_to_select).toPandas()\n",
        "\n",
        "# Add derived columns for Property Type, Room Type, and Revenue\n",
        "df['Property Type'] = df[[\n",
        "    \"property_type_Entire_Unit\",\n",
        "    \"property_type_Cultural_&_Local_Experiences\",\n",
        "    \"property_type_Other\",\n",
        "    \"property_type_Vehicles_as_Accommodation\",\n",
        "    \"property_type_Unique_Stays\",\n",
        "    \"property_type_Farm_&_Ranch_Stays\",\n",
        "    \"property_type_Shared_Accommodation\",\n",
        "    \"property_type_Nature-Based_Stays\",\n",
        "    \"property_type_Private_Room\"\n",
        "]].idxmax(axis=1).str.replace('property_type_', '', regex=False)\n",
        "\n",
        "df['Room Type'] = df[[\n",
        "    \"room_type_Shared_room\",\n",
        "    \"room_type_Hotel_room\",\n",
        "    \"room_type_Entire_home/apt\",\n",
        "    \"room_type_Private_room\"\n",
        "]].idxmax(axis=1).str.replace('room_type_', '', regex=False)\n",
        "\n",
        "df['Revenue'] = (90 - df['listings_availability_90']) * df['listings_price']\n",
        "\n",
        "# Precompute pivot data\n",
        "pivot_data = df.groupby(['listings_ZIPCODE', 'Property Type', 'Room Type', 'bathrooms_count'])['Revenue'].mean().reset_index()\n",
        "pivot_data.rename(columns={'Revenue': 'Average Revenue Generated'}, inplace=True)\n",
        "\n",
        "# Initialize Dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Dropdown options\n",
        "zip_options = [{\"label\": str(zip_code), \"value\": zip_code} for zip_code in df[\"listings_ZIPCODE\"].unique()]\n",
        "\n",
        "# App layout\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Dynamic Pivot Table by ZIP Code\"),\n",
        "    html.Label(\"Select a ZIP Code:\"),\n",
        "    dcc.Dropdown(\n",
        "        id=\"zip-dropdown\",\n",
        "        options=zip_options,\n",
        "        value=df[\"listings_ZIPCODE\"].unique()[0]\n",
        "    ),\n",
        "    html.Div(id=\"pivot-table\")\n",
        "])\n",
        "\n",
        "# Callback to update pivot table based on selected ZIP Code\n",
        "@app.callback(\n",
        "    Output(\"pivot-table\", \"children\"),\n",
        "    [Input(\"zip-dropdown\", \"value\")]\n",
        ")\n",
        "def update_pivot_table(selected_zip):\n",
        "    # Filter precomputed pivot data\n",
        "    filtered_data = pivot_data[pivot_data[\"listings_ZIPCODE\"] == selected_zip].head(10)\n",
        "\n",
        "    # Render pivot table using DataTable\n",
        "    return html.Div([\n",
        "        html.H3(f\"Pivot Table for ZIP Code: {selected_zip}\"),\n",
        "        DataTable(\n",
        "            data=filtered_data.to_dict(\"records\"),\n",
        "            columns=[\n",
        "                {\"name\": \"Property Type\", \"id\": \"Property Type\"},\n",
        "                {\"name\": \"Room Type\", \"id\": \"Room Type\"},\n",
        "                {\"name\": \"Bathroom Count\", \"id\": \"bathrooms_count\"},\n",
        "                {\"name\": \"Average Revenue Generated\", \"id\": \"Average Revenue Generated\"}\n",
        "            ],\n",
        "            style_table={\"height\": \"400px\", \"overflowY\": \"auto\"},\n",
        "            style_cell={\"textAlign\": \"center\", \"fontSize\": 10},\n",
        "            style_header={\"backgroundColor\": \"lightgrey\", \"fontWeight\": \"bold\"}\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.run_server(debug=True, port=8051)\n"
      ],
      "metadata": {
        "id": "l28vLzf4alCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}